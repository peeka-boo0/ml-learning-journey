{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBMCPq0UZiDz4b4ssH/oTg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peeka-boo0/ml-learning-journey/blob/main/notebooks/notebook_2/Day_17_boosting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1Ô∏è‚É£ Bagging (Bootstrap Aggregating)\n",
        "\n",
        "Train many models independently on random subsets of the data.\n",
        "\n",
        "Each model votes (classification) or averages (regression).\n",
        "\n",
        "Goal = reduce variance (make predictions stable).\n",
        "\n",
        "Example: Random Forest\n",
        "\n",
        "üí° Think: ‚Äúmany trees trained separately, then majority vote.‚Äù\n",
        "\n",
        "2Ô∏è‚É£ Boosting\n",
        "\n",
        "Train models sequentially, each new model fixes the errors of the last one.\n",
        "\n",
        "Goal = reduce bias (make predictions smarter).\n",
        "\n",
        "Examples: AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost.\n",
        "\n",
        "üí° Think: ‚Äúone tree learns, then next tree improves the mistakes.‚Äù\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## üå≥ Boosting Types (Easy Comparison)\n",
        "\n",
        "| Method                           | How it Works                                                                                                                                                           | Pros                                 | Cons                                              |\n",
        "| -------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------ | ------------------------------------------------- |\n",
        "| **AdaBoost** (Adaptive Boosting) | Starts with a simple weak learner (usually stumps = depth-1 trees). In each round, it **increases weight** on wrongly classified samples so next tree focuses on them. | Simple, works well on clean data.    | Sensitive to noise (outliers get too much focus). |\n",
        "| **Gradient Boosting**            | Each new tree learns from the **errors (residuals)** of the last one, using gradient descent to minimize loss.                                                         | Flexible, can optimize many losses.  | Can overfit if too many trees.                    |\n",
        "| **XGBoost** (Extreme GB)         | Same as gradient boosting, but with **regularization** (to prevent overfit), **parallel training**, and **fast handling of missing data**.                             | Faster + better generalization.      | More hyperparams (tuning needed).                 |\n",
        "| **LightGBM**                     | Uses **histograms + leaf-wise growth** ‚Üí grows trees faster and deeper. Great for very large datasets.                                                                 | Extremely fast, handles huge data.   | Can overfit small data.                           |\n",
        "| **CatBoost**                     | Specially optimized for **categorical features** (like gender, city, color) without needing manual encoding.                                                           | Best for categorical-heavy datasets. | Training can be slower than LightGBM.             |\n",
        "\n",
        "---\n",
        "\n",
        "## üìä XGBoost Metrics\n",
        "\n",
        "### üîπ Classification\n",
        "\n",
        "| Metric       | Meaning                                                                       | When to Use                                             |\n",
        "| ------------ | ----------------------------------------------------------------------------- | ------------------------------------------------------- |\n",
        "| **error**    | % of wrong predictions (1 - accuracy). Lower = better.                        | Quick check, balanced data.                             |\n",
        "| **logloss**  | Considers both correctness and confidence of prediction. Lower = better.      | Multi-class classification (e.g., digits).              |\n",
        "| **auc**      | Ability to rank positive vs negative correctly (0.5 = random, 1.0 = perfect). | Imbalanced binary classification (fraud, disease).      |\n",
        "| **mlogloss** | Multi-class version of logloss.                                               | Multi-class problems (digits, images, text categories). |\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Regression\n",
        "\n",
        "| Metric    | Meaning                                                                   | When to Use                                                            |\n",
        "| --------- | ------------------------------------------------------------------------- | ---------------------------------------------------------------------- |\n",
        "| **rmse**  | Square-root of average squared error. Big errors hurt more.               | When big mistakes are critical (house prices).                         |\n",
        "| **mae**   | Average absolute error (treats all errors equally).                       | When all errors are equally bad.                                       |\n",
        "| **rmsle** | Root mean squared log error (reduces impact of very large target values). | When target values vary a lot (like predicting population or revenue). |\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **Quick rule**:\n",
        "\n",
        "* Balanced classes ‚Üí `error` / `logloss`\n",
        "* Imbalanced classes ‚Üí `auc`\n",
        "* Regression with outliers ‚Üí `rmse`\n",
        "* Regression stable ‚Üí `mae`\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Nu-SuWOB38V_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ULxvFFzUiby2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5780c2d0-9e50-4a8f-848f-b88877d62099"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9666666666666667\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        33\n",
            "           1       0.93      0.96      0.95        28\n",
            "           2       0.97      1.00      0.99        33\n",
            "           3       1.00      0.94      0.97        34\n",
            "           4       1.00      0.93      0.97        46\n",
            "           5       0.96      0.96      0.96        47\n",
            "           6       0.97      0.94      0.96        35\n",
            "           7       0.97      0.97      0.97        34\n",
            "           8       0.97      1.00      0.98        30\n",
            "           9       0.91      0.97      0.94        40\n",
            "\n",
            "    accuracy                           0.97       360\n",
            "   macro avg       0.97      0.97      0.97       360\n",
            "weighted avg       0.97      0.97      0.97       360\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load data\n",
        "digits = load_digits()\n",
        "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=42) #spliting the data\n",
        "\n",
        "# Train XGBoost\n",
        "xgb = XGBClassifier(n_estimators=50, learning_rate=1.0, max_depth=3, eval_metric='mlogloss')\n",
        "xgb.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Case 1: High learning rate, few trees(fast but less acc)\n",
        "#xgb1 = XGBClassifier(n_estimators=50, learning_rate=1.0, max_depth=3, eval_metric='mlogloss')\n",
        "#xgb1.fit(X_train, y_train)\n",
        "#print(\"High LR Accuracy:\", accuracy_score(y_test, xgb1.predict(X_test)))\n",
        "\n",
        "# Case 2: Low learning rate, many trees (slow but more acc)\n",
        "#xgb2 = XGBClassifier(n_estimators=500, learning_rate=0.05, max_depth=3, eval_metric='mlogloss')\n",
        "#xgb2.fit(X_train, y_train)\n",
        "#print(\"Low LR Accuracy:\", accuracy_score(y_test, xgb2.predict(X_test)))\n",
        "\n",
        "# Predict\n",
        "y_pred = xgb.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#practice problem for finding acc diffrence b/w xgb boosting and the normal randomforest model\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "\n",
        "#loding the data\n",
        "digits = load_digits()\n",
        "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=42)\n",
        "\n",
        "#inishlizing the models\n",
        "\n",
        "#model 1\n",
        "rf = RandomForestClassifier(n_estimators=50,max_depth=3,random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "#model 2\n",
        "xgb = XGBClassifier(n_estimators=50, learning_rate=1.0, max_depth=3, eval_metric='mlogloss')\n",
        "xgb.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "#Geying the pridictions\n",
        "\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "y_pred_xgb = xgb.predict(X_test)\n",
        "\n",
        "#Evaluating the models\n",
        "\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "print(\"XGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
        "print(classification_report(y_test, y_pred_xgb))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cV7bQ19G5-Jx",
        "outputId": "08954d2f-318c-4bf1-ddad-289135b340ed"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 0.8833333333333333\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.96        33\n",
            "           1       0.87      0.71      0.78        28\n",
            "           2       0.83      0.91      0.87        33\n",
            "           3       0.84      0.94      0.89        34\n",
            "           4       0.98      0.91      0.94        46\n",
            "           5       0.98      0.85      0.91        47\n",
            "           6       0.87      0.97      0.92        35\n",
            "           7       0.79      1.00      0.88        34\n",
            "           8       0.91      0.70      0.79        30\n",
            "           9       0.82      0.82      0.82        40\n",
            "\n",
            "    accuracy                           0.88       360\n",
            "   macro avg       0.88      0.88      0.88       360\n",
            "weighted avg       0.89      0.88      0.88       360\n",
            "\n",
            "XGBoost Accuracy: 0.9666666666666667\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        33\n",
            "           1       0.93      0.96      0.95        28\n",
            "           2       0.97      1.00      0.99        33\n",
            "           3       1.00      0.94      0.97        34\n",
            "           4       1.00      0.93      0.97        46\n",
            "           5       0.96      0.96      0.96        47\n",
            "           6       0.97      0.94      0.96        35\n",
            "           7       0.97      0.97      0.97        34\n",
            "           8       0.97      1.00      0.98        30\n",
            "           9       0.91      0.97      0.94        40\n",
            "\n",
            "    accuracy                           0.97       360\n",
            "   macro avg       0.97      0.97      0.97       360\n",
            "weighted avg       0.97      0.97      0.97       360\n",
            "\n"
          ]
        }
      ]
    }
  ]
}