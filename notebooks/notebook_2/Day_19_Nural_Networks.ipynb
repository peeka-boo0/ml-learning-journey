{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPP3s9TFSgvyPlNk57KAtEz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peeka-boo0/ml-learning-journey/blob/main/notebooks/notebook_2/Day_19_Nural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UmDQ5UlIYTm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceee24e1-32b4-4c7a-b638-90941a9afc72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural Network Accuracy: 0.9666666666666667\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.97      0.98        33\n",
            "           1       0.93      0.96      0.95        28\n",
            "           2       0.94      0.97      0.96        33\n",
            "           3       1.00      0.97      0.99        34\n",
            "           4       0.98      1.00      0.99        46\n",
            "           5       0.96      0.98      0.97        47\n",
            "           6       0.94      0.97      0.96        35\n",
            "           7       1.00      0.97      0.99        34\n",
            "           8       0.93      0.90      0.92        30\n",
            "           9       0.97      0.95      0.96        40\n",
            "\n",
            "    accuracy                           0.97       360\n",
            "   macro avg       0.97      0.96      0.97       360\n",
            "weighted avg       0.97      0.97      0.97       360\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load data\n",
        "digits = load_digits()\n",
        "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a simple Neural Network\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(50,), activation='relu', solver='adam', max_iter=300, random_state=42) #the auto adam is a wight assiner\n",
        "#the(50,) is defining one nurone layer with 50 nurones\n",
        "#the relu squise or reshape the outputs to 0-x\n",
        "#'logistic'  squashes output 0–1.\n",
        "#'tanh' → squashes output -1 to 1.\n",
        "#'identity' → no transform the output data stays the same\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "mlp.fit(X_train, y_train)\n",
        "# Predictions\n",
        "y_pred = mlp.predict(X_test)\n",
        "\n",
        "# Results\n",
        "print(\"Neural Network Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#neural network by tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler #convert feactures values around 0,1 by standerdeviation and mean so The neural network dont think height is more important just because its numbers are bigger.\n",
        "from tensorflow.keras.utils import to_categorical #conver the labes to one hot vector so the nural network dont thik they are related eg- 1=(0,0,1),2=(0,1,0),3=(1,0,0)\n",
        "\n",
        "# Load data\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "# Normalize inputs\n",
        "scaler = StandardScaler()#also make the model fast\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build NN model\n",
        "model = models.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(64,)),  # 64 input features\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')  # 10 output classes & give problity distrebustion accross 0-9\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) #the loss is finding the hoe wrong the moled pridiction is and then the optimizer is givig wight to them accordinly\n",
        "\n",
        "# Train\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1) #batch size 32 it divide the data onto 32 for eg- 320 /32 = 10 per batch after 32 batch one epochs is done epochs mean going though data at onecs\n",
        "\n",
        "# Evaluate\n",
        "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Keras NN Accuracy: {acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsiuAcsWOY2S",
        "outputId": "530adc71-e0e1-49e6-dfe3-687a3ed3ecb5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.1716 - loss: 2.3359\n",
            "Epoch 2/20\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6886 - loss: 1.3575\n",
            "Epoch 3/20\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8751 - loss: 0.6262\n",
            "Epoch 4/20\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9427 - loss: 0.3168\n",
            "Epoch 5/20\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9645 - loss: 0.1998\n",
            "Epoch 6/20\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9813 - loss: 0.1276\n",
            "Epoch 7/20\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9850 - loss: 0.1023\n",
            "Epoch 8/20\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9937 - loss: 0.0711\n",
            "Epoch 9/20\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9914 - loss: 0.0658\n",
            "Epoch 10/20\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9958 - loss: 0.0484\n",
            "Epoch 11/20\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9930 - loss: 0.0479\n",
            "Epoch 12/20\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9967 - loss: 0.0368\n",
            "Epoch 13/20\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9974 - loss: 0.0310\n",
            "Epoch 14/20\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9957 - loss: 0.0289\n",
            "Epoch 15/20\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9966 - loss: 0.0255\n",
            "Epoch 16/20\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9979 - loss: 0.0238\n",
            "Epoch 17/20\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9997 - loss: 0.0166\n",
            "Epoch 18/20\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9994 - loss: 0.0139\n",
            "Epoch 19/20\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0126\n",
            "Epoch 20/20\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0119\n",
            "Keras NN Accuracy: 0.9694\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "# Normalize\n",
        "scaler = StandardScaler()#feactures fit so the model dont interrelate there values weight\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Convert to tensors\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)#converting the x data to float cozz the feactures have decimel values\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)#converting the y data to long(integer) cozz the labels dont have decimel values\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Define model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(64, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleNN() #the nural model\n",
        "\n",
        "# Loss & optimizer\n",
        "criterion = nn.CrossEntropyLoss()#loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)#weight optimizer\n",
        "\n",
        "# Training loop\n",
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Evaluation\n",
        "with torch.no_grad():\n",
        "    y_pred = model(X_test)\n",
        "    y_pred_classes = torch.argmax(y_pred, dim=1)\n",
        "    acc = accuracy_score(y_test, y_pred_classes)\n",
        "\n",
        "print(f\"PyTorch NN Accuracy: {acc:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aCDhZphcWsT",
        "outputId": "1d3261c6-3406-413e-ae94-ce29ea78de51"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/20], Loss: 2.2365\n",
            "Epoch [10/20], Loss: 2.1567\n",
            "Epoch [15/20], Loss: 2.0638\n",
            "Epoch [20/20], Loss: 1.9518\n",
            "PyTorch NN Accuracy: 0.5917\n"
          ]
        }
      ]
    }
  ]
}