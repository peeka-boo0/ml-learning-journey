{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNahvk/BSSEqrvfRrN8OHAa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peeka-boo0/ml-learning-journey/blob/main/notebooks/Day_13_01_Different_kfolds.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "### 1. **KFold**\n",
        "\n",
        "* Splits the dataset into *k* folds (e.g., 5).\n",
        "* In each round, one fold is the test set, others are train.\n",
        "* Does **not guarantee class balance** in each fold.\n",
        "* ‚ö†Ô∏è Risk: if your dataset is imbalanced (like 90% zeros, 10% ones), some folds might miss the minority class completely.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **StratifiedKFold**\n",
        "\n",
        "* Same as KFold, but it **preserves the class distribution** (ratios of each label) in every fold.\n",
        "* Much better for classification problems, especially with imbalanced data.\n",
        "* Example: If your dataset has 30% ‚Äúdog‚Äù and 70% ‚Äúcat,‚Äù then every fold will keep that \\~30/70 ratio.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Leave-One-Out (LOO)**\n",
        "\n",
        "* Extreme case of KFold where **k = number of samples**.\n",
        "* Each fold uses exactly 1 sample as the test set, and the rest as training.\n",
        "* Very thorough, but computationally **super expensive** for large datasets.\n",
        "* Used when dataset is very small (like medical datasets).\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **ShuffleSplit**\n",
        "\n",
        "* Randomly splits the dataset into train/test multiple times.\n",
        "* You can set the train/test size (e.g., 80% train, 20% test).\n",
        "* Doesn‚Äôt require strict ‚Äúfolds‚Äù ‚Üí more flexible.\n",
        "* Useful when you want random resampling instead of strict partitioning.\n",
        "\n",
        "---\n",
        "\n",
        "üëâ Summary for your notes:\n",
        "\n",
        "* **KFold** ‚Üí simple splitting, may miss minority classes.\n",
        "* **StratifiedKFold** ‚Üí keeps class ratios balanced (default for classification).\n",
        "* **Leave-One-Out** ‚Üí one test sample per fold, very slow but precise.\n",
        "* **ShuffleSplit** ‚Üí random splits, flexible, can control train/test ratio.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "E7EbSYDnMKIX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O98zejWUJb7e",
        "outputId": "dc841b73-4a44-4dd9-b8d2-b1866fa13f2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KFold mean accuracy: 0.9771804394924171\n",
            "StratifiedKFold mean accuracy: 0.9788502011761067\n",
            "ShuffleSplit mean accuracy: 0.9755555555555556\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, ShuffleSplit\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "digits = load_digits()\n",
        "X, y = digits.data, digits.target\n",
        "\n",
        "# Model\n",
        "rf = RandomForestClassifier(n_estimators=200, max_depth=20, random_state=42)\n",
        "\n",
        "# 1. Simple KFold (no stratification)\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores_kf = cross_val_score(rf, X, y, cv=kf)\n",
        "\n",
        "# 2. StratifiedKFold (keeps class balance)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores_skf = cross_val_score(rf, X, y, cv=skf)\n",
        "\n",
        "# 3. ShuffleSplit (random train/test splits)\n",
        "ss = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
        "scores_ss = cross_val_score(rf, X, y, cv=ss)\n",
        "\n",
        "print(\"KFold mean accuracy:\", np.mean(scores_kf))\n",
        "print(\"StratifiedKFold mean accuracy:\", np.mean(scores_skf))\n",
        "print(\"ShuffleSplit mean accuracy:\", np.mean(scores_ss))\n"
      ]
    }
  ]
}